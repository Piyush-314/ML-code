# Neural network:
    Make a prediction
    Measure error (loss)
    Use calculus to adjust weights to reduce error

    Neuron:
        z=wx+b
        y^â€‹=f(z)

    ğ‘¤ (w = weight)
    ğ‘(b = bias)
    ğ‘“(f = activation function)
    composition of functions: xâ†’zâ†’y^


    Loss fxn:
        Mean Squared Error: L=(y^â€‹âˆ’y)^2
        chain: (w â†’ z â†’ y^ â€‹â†’ L)


Backpropogation:
    Neural networks are nested functions: (Loss â†’ Output â†’ Activation â†’ Weights)
    Applying the chain rule backward through the network
    y=f(g(x))
    dy/dx = dy/dg . dg/dx

    We want: min L (minimise the loss)

    We want: âˆ‚ğ¿/âˆ‚ğ‘¤
    âˆ‚ğ¿/âˆ‚ğ‘¤ = (âˆ‚ğ¿/âˆ‚y^) * (âˆ‚y^/âˆ‚z) * (âˆ‚z/âˆ‚w)


    # Step 1: Loss derivative
        L = (y^â€‹âˆ’y)^2
        âˆ‚ğ¿/âˆ‚y^ = 2(y^â€‹âˆ’y)

    # Step 2: Activation derivative
        y^ = ğœ(z)
        âˆ‚y^/âˆ‚z = Ïƒ(z)(1âˆ’Ïƒ(z))

    # Step 3: Linear derivative
        z = wx + b
        âˆ‚z/âˆ‚w = x

    # Final
        âˆ‚ğ¿/âˆ‚ğ‘¤ = 2(y^â€‹âˆ’y) * Ïƒ(z)(1âˆ’Ïƒ(z)) * x


# Gradient Descent
    An algorithm to minimize a function (downwar descent    )
    Î¸=Î¸âˆ’Î±âˆ‡L

    Î¸ = parameters (weights)
    Î± = learning rate
    âˆ‡L = gradient (vector of slope)  âˆ‡L=[âˆ‚w/âˆ‚L â€‹âˆ‚b/âˆ‚Lâ€‹â€‹]
	â€‹
    w = w âˆ’ Î±.(âˆ‚w/âˆ‚L)â€‹
    b = b âˆ’ Î±.(âˆ‚b/âˆ‚L)â€‹


#
| Function | Formula        | Derivative         |
| -------- | -------------- | ------------------ |
| Sigmoid  | (1/(1+e^{-x})) | (Ïƒ(x)(1-Ïƒ(x)))     |
| ReLU     | (\max(0,x))    | 1 if (x>0), else 0 |
| Tanh     | (\tanh(x))     | (1-\tanh^2(x))     |

